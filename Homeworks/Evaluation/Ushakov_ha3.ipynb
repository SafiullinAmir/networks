{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Network Science</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Home Assignment #3: Centralities and Assortativity</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Student: Maxim Ushakov</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <hr /> General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due Date:** 13.03.2016 23:59 <br \\>\n",
    "**Late submission policy:** -0.2 points per day <br \\>\n",
    "\n",
    "\n",
    "Please send your reports to <mailto:network.hse.2016@gmail.com> with message subject of the following structure:<br \\> **[HSE Networks 2015] *{LastName}* *{First Name}* HA*{Number}***\n",
    "\n",
    "Support your computations with figures and comments. <br \\>\n",
    "If you are using IPython Notebook you may use this file as a starting point of your report.<br \\>\n",
    "<br \\>\n",
    "<hr \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute degree centrality, Pagerank and  HubAuthorities scores for the [flickr](https://snap.stanford.edu/data/web-flickr.html) network. \n",
    "\n",
    "Data contains sparse matrix A and list of user names.\n",
    "This is a “denser” part of the Flickr photo sharing site friendship graph from 2006. Edge direction corresponds to friendship requests (following). Some of the links are reciprocal,others not.  \n",
    "\n",
    "Provide top 50 names in each ranking, compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "We start with Pagerank score, then we calculate Hub and Authorities cores, then we compute incoming and outcoming degree centralities, and, finally, we compare obtained results.\n",
    "Let us download the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "data = scipy.io.loadmat('flickr.mat')\n",
    "A = data['A']\n",
    "names = data['names']\n",
    "num_vertices = A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out that all vertices have outcoming edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15724\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.array(A.sum(1)>0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all vertices have outcoming edges (there is no absorbing node in our graph), than this graph satisfies Perron-Frobenius theorem (without any modifications). We use the formula:\n",
    "$$p=\\alpha \\cdot (D^{-1}\\cdot A)^T\\cdot p + (1-\\alpha)\\cdot e/n$$,\n",
    "to compute Pagerank score.\n",
    "By setting regularization coefficient equal to 0.999, we obtain matrices $B=\\alpha \\cdot (D^{-1}\\cdot A)^T$ and $C=(1-\\alpha)\\cdot e/n$. We will need them to compute Pagerank score $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.999\n",
    "P = scipy.sparse.lil_matrix(A)\n",
    "D_diag = A.sum(1)\n",
    "for i in range(num_vertices):\n",
    "    P[i,] /= D_diag[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagerank_score = np.ones(num_vertices)/num_vertices\n",
    "B = alpha*P.transpose()\n",
    "C = (1-alpha)*np.ones(num_vertices)/num_vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've tried to find $p$ by solving system $(I-B)\\cdot p=C$, but, unfortunately, jupyter's kernel has died during computations, so I've decided to solve the system iteratively. To ensure uniform convergence of $p$ to real Pagerank score, we set up maximal error value $eps \\approx 1/30000$. Through 10 iterations we obtain Pagerank score with given accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.00216565412268\n",
      "2 0.000380384142822\n",
      "3 0.000137013658883\n",
      "4 6.97535184881e-05\n",
      "5 5.05494776548e-05\n",
      "6 4.25222718043e-05\n",
      "7 3.87872010842e-05\n",
      "8 3.55731338804e-05\n",
      "9 3.31503206455e-05\n",
      "10 3.08481563947e-05\n"
     ]
    }
   ],
   "source": [
    "pr_pagerank_score = np.zeros(num_vertices)/num_vertices\n",
    "eps = 1.0 / (2*num_vertices)\n",
    "\n",
    "num_try = 0\n",
    "while np.max(np.abs(pagerank_score - pr_pagerank_score)) > eps:\n",
    "    pr_pagerank_score = pagerank_score\n",
    "    pagerank_score = B*pagerank_score + C\n",
    "    \n",
    "    num_try += 1\n",
    "    print(num_try, np.max(np.abs(pagerank_score - pr_pagerank_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have names of 50 users with highest Pagerank score (presented in descending order). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awfulsara\n",
      "drp\n",
      "BombDog\n",
      "antimethod\n",
      "*Ivan*\n",
      "DrJoanne\n",
      "Simon Pais\n",
      "cymagen\n",
      "deborah lattimore\n",
      ":Nikola\n",
      "slowernet\n",
      "jkottke\n",
      "notraces\n",
      "artofgold\n",
      "MaD GiÂ®Lâ¢â¢\n",
      "Pandarine\n",
      "lorrainemd\n",
      "romanlily\n",
      "*starlet*\n",
      "fraying\n",
      "anildash\n",
      "Mylens\n",
      "gu@va\n",
      "underbunny\n",
      "Mareen Fischinger\n",
      "maximolly\n",
      "Loobylu\n",
      "* HoNe$t *\n",
      "Joi\n",
      "pbowers\n",
      "Tom Coates\n",
      "gruntzooki\n",
      "pixietart\n",
      "bernardo.borghetti\n",
      "chromogenic\n",
      "Gayla\n",
      "Departure Lounge\n",
      "Marcelo  Montecino\n",
      "overshadowed\n",
      "Esther_G\n",
      "jakedobkin\n",
      "aleyna\n",
      "990000\n",
      "CherryVega\n",
      "hot_luscious\n",
      "rion\n",
      "macwagen\n",
      "chacabuco\n",
      "pablokorona\n",
      "Agridulce\n"
     ]
    }
   ],
   "source": [
    "names = [name.strip(' ') for name in names]\n",
    "pagerank_score_names = [(names[i], pagerank_score[i]) for i in range(num_vertices)]\n",
    "sorted_names_pagerank = sorted(pagerank_score_names, key=lambda obj: obj[1], reverse=True)\n",
    "for name in sorted_names_pagerank[:50]:\n",
    "    print(name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute Hub and Authority scores we use formulas:\n",
    "$$(A^T A) \\cdot a = \\lambda \\cdot a,$$\n",
    "$$(A A^T) \\cdot h = \\lambda \\cdot h,$$\n",
    "where $\\lambda$ is a maximum eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_temp = A*A.transpose()\n",
    "l, h_vector = scipy.sparse.linalg.eigs(A_temp,k=1)\n",
    "A_temp = A.transpose()*A\n",
    "l2, a_vector = scipy.sparse.linalg.eigs(A_temp,k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out that maximum eigenvalues are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6153.46853672+0.j] [ 6153.46853672+0.j]\n"
     ]
    }
   ],
   "source": [
    "print(l, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have names of 50 users with highest Hub score (presented in descending order). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "florecita120\n",
      "agrypnia\n",
      "Lutrus\n",
      "Fellowship of the Rich\n",
      "HikingVal\n",
      "francesgatz\n",
      "nomad7674\n",
      "Princess Lou\n",
      "Karl Kanal\n",
      "Jacki Gordon\n",
      "Bonnie Christiana (Quibell)\n",
      "Belinda 05\n",
      "Miss CC\n",
      "jcruelty\n",
      "TheRealOdie\n",
      "hkboyee\n",
      "snkutty\n",
      "caffcaff\n",
      "heven2mrgatroid\n",
      "Sylla\n",
      "sulaco_rm\n",
      "misseill\n",
      "Termo\n",
      "qbqrat\n",
      "louloubelle2\n",
      "arlo\n",
      "MissNicky\n",
      "Gimelo\n",
      "hotdiggidydog\n",
      "slightlydope\n",
      "Kir0n\n",
      "_Yodi\n",
      "JimisonPix\n",
      "wasabi1809\n",
      "DawgPound\n",
      "Melanie Maynor\n",
      "Andrew Mac\n",
      "3a|M|atko0m 7ub 7ub\n",
      "waration\n",
      "speedyplatinum\n",
      "sazzz\n",
      "bobben\n",
      "twcheong\n",
      "MrKev\n",
      "AraBiC\n",
      "Zars\n",
      "eve_\n",
      "Nicki & Jase\n",
      "naleslie\n",
      "CRM\n"
     ]
    }
   ],
   "source": [
    "hub_score_names = [(names[i], h_vector[i]) for i in range(num_vertices)]\n",
    "sorted_names_hub = sorted(hub_score_names, key=lambda obj: obj[1], reverse=True)\n",
    "for name in sorted_names_hub[:50]:\n",
    "    print(name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have names of 50 users with highest Authority score (presented in descending order). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "francesgatz\n",
      "HikingVal\n",
      "florecita120\n",
      "caffcaff\n",
      "Bonnie Christiana (Quibell)\n",
      "Jacki Gordon\n",
      "Belinda 05\n",
      "Gimelo\n",
      "Miss CC\n",
      "babepy\n",
      "agrypnia\n",
      "snkutty\n",
      "Wati\n",
      "louloubelle2\n",
      "eingy\n",
      "sazzz\n",
      "speedyplatinum\n",
      "Awlad-Alhowa\n",
      "twcheong\n",
      "Extra Girl\n",
      "james and danielle\n",
      "MrKev\n",
      "DawgPound\n",
      "hotdiggidydog\n",
      "heven2mrgatroid\n",
      "Miss_Gossip\n",
      "ÒÃ¤ÅÄ·ÅÅÅÃ¤\n",
      "Lady Birchwood\n",
      "miss pepsi\n",
      "ShÃ´jo\n",
      "_Yodi\n",
      "MissNicky\n",
      "Justin B\n",
      "qbqrat\n",
      "Loefty\n",
      "TheRealOdie\n",
      "gigiloo\n",
      "TruckstopC\n",
      "AraBiC\n",
      "I'mMessy\n",
      "B...\n",
      "misseill\n",
      "mohammed3\n",
      "slightlydope\n",
      "KaiFy_EmaRaTy\n",
      "Melanie Maynor\n",
      "waration\n",
      "FraukeFoto\n",
      "*&#@UniQuE@#&*\n",
      "(Â¯`Â·._.Â·[Mr.MaZyo0oN]Â·._.Â·Â´Â¯)\n"
     ]
    }
   ],
   "source": [
    "authority_score_names = [(names[i], a_vector[i]) for i in range(num_vertices)]\n",
    "sorted_names_authority = sorted(authority_score_names, key=lambda obj: obj[1], reverse=True)\n",
    "for name in sorted_names_authority[:50]:\n",
    "    print(name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate indegree and outdegree centrality (simply as number of incoming/outcoming vertices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outdegree_centrality = A.sum(1)\n",
    "indegree_centrality = A.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have names of 50 users with highest outdegree centrality (presented in descending order). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anildash\n",
      "tozzer\n",
      "AtiRanA\n",
      "pixietart\n",
      "jakedobkin\n",
      "Buntekuh\n",
      "brainware3000\n",
      "Jakes_World\n",
      "maximolly\n",
      "Andreia Lopes\n",
      "elvis1967\n",
      "inthegan\n",
      "990000\n",
      ": Esther\n",
      "dmoore\n",
      "kathryn\n",
      "Charles Machado\n",
      "spanier\n",
      "drp\n",
      "What What\n",
      "rivello\n",
      "mehmetkale\n",
      "blackbeltjones\n",
      "dianeham\n",
      "Lucilia\n",
      "meadows\n",
      "aracay\n",
      "pro keds\n",
      "a nameless yeast\n",
      "cooks\n",
      "nickf\n",
      "virgu\n",
      "tantek\n",
      "Zsaj\n",
      "Photographer Dre\n",
      "jayallen\n",
      "cymagen\n",
      "gardengal\n",
      "me4ta\n",
      "mypapercrane\n",
      "judith\n",
      "Airchild\n",
      "ribena\n",
      "roamin\n",
      "mdintoronto\n",
      "lucycat\n",
      "killer robot\n",
      "chomp\n",
      "Waldgeister\n",
      "ghostbones\n"
     ]
    }
   ],
   "source": [
    "outdegree_score_names = [(names[i], outdegree_centrality[i]) for i in range(num_vertices)]\n",
    "sorted_names_outdegree = sorted(outdegree_score_names, key=lambda obj: obj[1], reverse=True)\n",
    "for name in sorted_names_outdegree[:50]:\n",
    "    print(name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have names of 50 users with highest indegree centrality (presented in descending order). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awfulsara\n",
      "drp\n",
      "*Ivan*\n",
      "antimethod\n",
      "DrJoanne\n",
      "BombDog\n",
      "Simon Pais\n",
      "deborah lattimore\n",
      "MaD GiÂ®Lâ¢â¢\n",
      "notraces\n",
      ":Nikola\n",
      "cymagen\n",
      "aleyna\n",
      "lorrainemd\n",
      "artofgold\n",
      "*starlet*\n",
      "romanlily\n",
      "Pandarine\n",
      "jkottke\n",
      "hot_luscious\n",
      "Mareen Fischinger\n",
      "Mylens\n",
      "slowernet\n",
      "gu@va\n",
      "bernardo.borghetti\n",
      "CherryVega\n",
      "pbowers\n",
      "underbunny\n",
      "Loobylu\n",
      "Merina\n",
      "* HoNe$t *\n",
      ".lush\n",
      "fraying\n",
      "pixietart\n",
      "naftalina007\n",
      "fd\n",
      "aquanerds\n",
      "Joi\n",
      "Esther_G\n",
      "carf\n",
      "Marcelo  Montecino\n",
      "!!uAe prince!!\n",
      "Sexy Swedish Babe\n",
      "callipygian\n",
      "Agridulce\n",
      "Least Wanted\n",
      "Gayla\n",
      "anildash\n",
      "tecgirl\n",
      "reddirtrose\n"
     ]
    }
   ],
   "source": [
    "indegree_score_names = [(names[i], indegree_centrality[0,i]) for i in range(num_vertices)]\n",
    "sorted_names_indegree = sorted(indegree_score_names, key=lambda obj: obj[1], reverse=True)\n",
    "for name in sorted_names_indegree[:50]:\n",
    "    print(name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare results, we compute Spearman rank correlation coefficient between every pair of scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranking = {}\n",
    "for name in names:\n",
    "    ranking[name] = {'page': 0, 'hub': 0, 'auth': 0, 'indeg': 0, 'outdeg': 0}\n",
    "for ind in range(num_vertices):\n",
    "    ranking[sorted_names_pagerank[ind][0]]['page']=ind\n",
    "    ranking[sorted_names_hub[ind][0]]['hub']=ind\n",
    "    ranking[sorted_names_authority[ind][0]]['auth']=ind\n",
    "    ranking[sorted_names_indegree[ind][0]]['indeg']=ind\n",
    "    ranking[sorted_names_outdegree[ind][0]]['outdeg']=ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5139048999754795\n",
      "-0.7532878626704752\n",
      "0.9466561676018755\n",
      "0.757843223375695\n",
      "0.811425161790339\n",
      "-0.5346320999834844\n",
      "-0.7338858865326491\n",
      "-0.7922200536475691\n",
      "-0.705067233892226\n",
      "0.8144566272754413\n"
     ]
    }
   ],
   "source": [
    "corr = {'page-hub': 0, 'page-auth': 0, 'page-indeg': 0, 'page-outdeg': 0,\\\n",
    "        'hub-auth': 0, 'hub-indeg': 0, 'hub-outdeg': 0,\\\n",
    "        'auth-indeg': 0, 'auth-outdeg': 0,\\\n",
    "        'indeg-outdeg': 0}\n",
    "for name, ranks in ranking.items():\n",
    "    corr['page-hub'] += ranks['page']*ranks['hub']\n",
    "    corr['page-auth'] += ranks['page']*ranks['auth']\n",
    "    corr['page-indeg'] += ranks['page']*ranks['indeg']\n",
    "    corr['page-outdeg'] += ranks['page']*ranks['outdeg']\n",
    "    corr['hub-auth'] += ranks['hub']*ranks['auth']\n",
    "    corr['hub-indeg'] += ranks['hub']*ranks['indeg']\n",
    "    corr['hub-outdeg'] += ranks['hub']*ranks['outdeg']\n",
    "    corr['auth-indeg'] += ranks['auth']*ranks['indeg']\n",
    "    corr['auth-outdeg'] += ranks['auth']*ranks['outdeg']\n",
    "    corr['indeg-outdeg'] += ranks['indeg']*ranks['outdeg']\n",
    "\n",
    "middle_corr = num_vertices*(num_vertices-1)*(num_vertices-1)/4\n",
    "half_corr = num_vertices*(num_vertices+1)*(num_vertices-1)/12\n",
    "\n",
    "corr['page-hub'] = (corr['page-hub']-middle_corr)/half_corr\n",
    "corr['page-auth'] = (corr['page-auth']-middle_corr)/half_corr\n",
    "corr['page-indeg'] = (corr['page-indeg']-middle_corr)/half_corr\n",
    "corr['page-outdeg'] = (corr['page-outdeg']-middle_corr)/half_corr\n",
    "corr['hub-auth'] = (corr['hub-auth']-middle_corr)/half_corr\n",
    "corr['hub-indeg'] = (corr['hub-indeg']-middle_corr)/half_corr\n",
    "corr['hub-outdeg'] = (corr['hub-outdeg']-middle_corr)/half_corr\n",
    "corr['auth-indeg'] = (corr['auth-indeg']-middle_corr)/half_corr\n",
    "corr['auth-outdeg'] = (corr['auth-outdeg']-middle_corr)/half_corr\n",
    "corr['indeg-outdeg'] = (corr['indeg-outdeg']-middle_corr)/half_corr\n",
    "\n",
    "print(corr['page-hub'])\n",
    "print(corr['page-auth'])\n",
    "print(corr['page-indeg'])\n",
    "print(corr['page-outdeg'])\n",
    "print(corr['hub-auth'])\n",
    "print(corr['hub-indeg'])\n",
    "print(corr['hub-outdeg'])\n",
    "print(corr['auth-indeg'])\n",
    "print(corr['auth-outdeg'])\n",
    "print(corr['indeg-outdeg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is high correlation between Pagerank score and indegree/outdegree centralities. It seems reasonable, as Pagerank score shows probability for node to be visited during random walk, and if node has many neighbors, then its probability to be visited is high. Also we can see that there is high relation between Hub and Authority scores. It also seems believable, as most popular nodes have high probability to be referenced, and vise versa, popular nodes are such nodes, that reference to other popular nodes. Finally, we can see negative correlation between Pagerank, indegree/outdegree scores and Hub and Authority scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <hr /> Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the [Facebook friendship graphs](https://snap.stanford.edu/data/egonets-Facebook.html) from several US universities from 2005 (one year after fb launch).\n",
    "\n",
    "Data contains a A matrix (sparse) and a \"local_info\" variable, one row per node: \n",
    "a student/faculty status flag, gender, major, second major/minor (if applicable), dorm/house, year, and high school. \n",
    "Missing data is coded 0.\n",
    "\n",
    "Compute node degree assortativity (mixining by node degree) and assortativity coefficient (modularity) for gender, major, dormitory, year, high school for all universities and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "Here we calculate degree assortativity and assortativity coefficients for every university's facebook network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with Berkeley university. First, we download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = scipy.io.loadmat('Berkeley13.mat')\n",
    "A2 = data2['A']\n",
    "local_info = data2['local_info']\n",
    "num_vertices2, num_attributes = local_info.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate degree assortativity by using the formula: \n",
    "$$r = \\frac{Se\\cdot S1-S2\\cdot S2}{S3\\cdot S1-S2\\cdot S2},$$\n",
    "where $S1 = 2m$, $S2 = \\sum_i k_i^2$, $S3 = \\sum_i k_i^3$, $Se = \\sum_{i,j} k_i\\cdot A_{i,j}\\cdot k_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01031346]]\n"
     ]
    }
   ],
   "source": [
    "k = np.array(A2.sum(1))\n",
    "S1 = k.sum()\n",
    "S2 = (k*k).sum()\n",
    "S3 = (k*k*k).sum()\n",
    "Se = np.matrix(k).transpose()*A2*np.matrix(k)\n",
    "r = (Se*S1-S2*S2)/(S3*S1-S2*S2)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate assortativity coefficients using the formula:\n",
    "$$\\frac{Q}{Q_{max}}=\\frac{\\sum_{i,j}(A_{ij}-\\frac{k_i k_j}{2m})\\delta (c_i,c_j)}{2m-\\sum_{i,j}(\\frac{k_i k_j}{2m})\\delta (c_i,c_j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: array([-0.80652993]), 2: array([-0.04454065]), 4: array([-0.61309089]), 5: array([-0.20304273]), 6: array([-0.01515052])}\n"
     ]
    }
   ],
   "source": [
    "attrib_check = [1,2,4,5,6]\n",
    "modularity = {1:0,2:0,4:0,5:0,6:0}\n",
    "for attr in attrib_check:\n",
    "    attr_classes = {}\n",
    "    #print(attr)\n",
    "    for j in range(num_vertices2):\n",
    "        try:\n",
    "            attr_classes[local_info[j, attr]].append(j)\n",
    "        except:\n",
    "            attr_classes[local_info[j, attr]] = [j]\n",
    "    #print(len(attr_classes))\n",
    "    max_modularity = S1\n",
    "    for attr_v, ind_list in attr_classes.items():\n",
    "        k_class = k[ind_list]\n",
    "        A_class = A2[ind_list,ind_list]\n",
    "        temp_sum = sum(k_class)**2/S1\n",
    "        max_modularity -= temp_sum\n",
    "        #print(temp_sum, max_modularity)\n",
    "        modularity[attr] += A_class.sum()-temp_sum\n",
    "    modularity[attr] /= max_modularity\n",
    "    #print(modularity[attr], max_modularity)\n",
    "    \n",
    "print(modularity)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same thing we do for other universities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06527295]]\n",
      "{1: array([-0.98182203]), 2: array([-0.08223703]), 4: array([-0.13632138]), 5: array([-0.27276695]), 6: array([-0.00889961])}\n"
     ]
    }
   ],
   "source": [
    "data2 = scipy.io.loadmat('Caltech36.mat')\n",
    "A2 = data2['A']\n",
    "local_info = data2['local_info']\n",
    "num_vertices2, num_attributes = local_info.shape\n",
    "k = np.array(A2.sum(1))\n",
    "S1 = k.sum()\n",
    "S2 = (k*k).sum()\n",
    "S3 = (k*k*k).sum()\n",
    "Se = np.matrix(k).transpose()*A2*np.matrix(k)\n",
    "r = (Se*S1-S2*S2)/(S3*S1-S2*S2)\n",
    "print(r)\n",
    "attrib_check = [1,2,4,5,6]\n",
    "modularity = {1:0,2:0,4:0,5:0,6:0}\n",
    "for attr in attrib_check:\n",
    "    attr_classes = {}\n",
    "    #print(attr)\n",
    "    for j in range(num_vertices2):\n",
    "        try:\n",
    "            attr_classes[local_info[j, attr]].append(j)\n",
    "        except:\n",
    "            attr_classes[local_info[j, attr]] = [j]\n",
    "    #print(len(attr_classes))\n",
    "    max_modularity = S1\n",
    "    for attr_v, ind_list in attr_classes.items():\n",
    "        k_class = k[ind_list]\n",
    "        A_class = A2[ind_list,ind_list]\n",
    "        temp_sum = sum(k_class)**2/S1\n",
    "        max_modularity -= temp_sum\n",
    "        #print(temp_sum, max_modularity)\n",
    "        modularity[attr] += A_class.sum()-temp_sum\n",
    "    modularity[attr] /= max_modularity\n",
    "    #print(modularity[attr], max_modularity)\n",
    "    \n",
    "print(modularity)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14505489]]\n",
      "{1: array([-0.72935764]), 2: array([-0.06568939]), 4: array([-0.07259287]), 5: array([-0.16229399]), 6: array([-0.03064139])}\n"
     ]
    }
   ],
   "source": [
    "data2 = scipy.io.loadmat('Harvard1.mat')\n",
    "A2 = data2['A']\n",
    "local_info = data2['local_info']\n",
    "num_vertices2, num_attributes = local_info.shape\n",
    "k = np.array(A2.sum(1))\n",
    "S1 = k.sum()\n",
    "S2 = (k*k).sum()\n",
    "S3 = (k*k*k).sum()\n",
    "Se = np.matrix(k).transpose()*A2*np.matrix(k)\n",
    "r = (Se*S1-S2*S2)/(S3*S1-S2*S2)\n",
    "print(r)\n",
    "attrib_check = [1,2,4,5,6]\n",
    "modularity = {1:0,2:0,4:0,5:0,6:0}\n",
    "for attr in attrib_check:\n",
    "    attr_classes = {}\n",
    "    #print(attr)\n",
    "    for j in range(num_vertices2):\n",
    "        try:\n",
    "            attr_classes[local_info[j, attr]].append(j)\n",
    "        except:\n",
    "            attr_classes[local_info[j, attr]] = [j]\n",
    "    #print(len(attr_classes))\n",
    "    max_modularity = S1\n",
    "    for attr_v, ind_list in attr_classes.items():\n",
    "        k_class = k[ind_list]\n",
    "        A_class = A2[ind_list,ind_list]\n",
    "        temp_sum = sum(k_class)**2/S1\n",
    "        max_modularity -= temp_sum\n",
    "        #print(temp_sum, max_modularity)\n",
    "        modularity[attr] += A_class.sum()-temp_sum\n",
    "    modularity[attr] /= max_modularity\n",
    "    #print(modularity[attr], max_modularity)\n",
    "    \n",
    "print(modularity)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07367326]]\n",
      "{1: array([-0.86190096]), 2: array([-0.02283702]), 4: array([-0.25421464]), 5: array([-0.20131707]), 6: array([-0.0154671])}\n"
     ]
    }
   ],
   "source": [
    "data2 = scipy.io.loadmat('Oklahoma97.mat')\n",
    "A2 = data2['A']\n",
    "local_info = data2['local_info']\n",
    "num_vertices2, num_attributes = local_info.shape\n",
    "k = np.array(A2.sum(1))\n",
    "S1 = k.sum()\n",
    "S2 = (k*k).sum()\n",
    "S3 = (k*k*k).sum()\n",
    "Se = np.matrix(k).transpose()*A2*np.matrix(k)\n",
    "r = (Se*S1-S2*S2)/(S3*S1-S2*S2)\n",
    "print(r)\n",
    "attrib_check = [1,2,4,5,6]\n",
    "modularity = {1:0,2:0,4:0,5:0,6:0}\n",
    "for attr in attrib_check:\n",
    "    attr_classes = {}\n",
    "    #print(attr)\n",
    "    for j in range(num_vertices2):\n",
    "        try:\n",
    "            attr_classes[local_info[j, attr]].append(j)\n",
    "        except:\n",
    "            attr_classes[local_info[j, attr]] = [j]\n",
    "    #print(len(attr_classes))\n",
    "    max_modularity = S1\n",
    "    for attr_v, ind_list in attr_classes.items():\n",
    "        k_class = k[ind_list]\n",
    "        A_class = A2[ind_list,ind_list]\n",
    "        temp_sum = sum(k_class)**2/S1\n",
    "        max_modularity -= temp_sum\n",
    "        #print(temp_sum, max_modularity)\n",
    "        modularity[attr] += A_class.sum()-temp_sum\n",
    "    modularity[attr] /= max_modularity\n",
    "    #print(modularity[attr], max_modularity)\n",
    "    \n",
    "print(modularity)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09109212]]\n",
      "{1: array([-0.76646806]), 2: array([-0.07374385]), 4: array([-0.11531597]), 5: array([-0.20038218]), 6: array([-0.0211872])}\n"
     ]
    }
   ],
   "source": [
    "data2 = scipy.io.loadmat('Princeton12.mat')\n",
    "A2 = data2['A']\n",
    "local_info = data2['local_info']\n",
    "num_vertices2, num_attributes = local_info.shape\n",
    "k = np.array(A2.sum(1))\n",
    "S1 = k.sum()\n",
    "S2 = (k*k).sum()\n",
    "S3 = (k*k*k).sum()\n",
    "Se = np.matrix(k).transpose()*A2*np.matrix(k)\n",
    "r = (Se*S1-S2*S2)/(S3*S1-S2*S2)\n",
    "print(r)\n",
    "attrib_check = [1,2,4,5,6]\n",
    "modularity = {1:0,2:0,4:0,5:0,6:0}\n",
    "for attr in attrib_check:\n",
    "    attr_classes = {}\n",
    "    #print(attr)\n",
    "    for j in range(num_vertices2):\n",
    "        try:\n",
    "            attr_classes[local_info[j, attr]].append(j)\n",
    "        except:\n",
    "            attr_classes[local_info[j, attr]] = [j]\n",
    "    #print(len(attr_classes))\n",
    "    max_modularity = S1\n",
    "    for attr_v, ind_list in attr_classes.items():\n",
    "        k_class = k[ind_list]\n",
    "        A_class = A2[ind_list,ind_list]\n",
    "        temp_sum = sum(k_class)**2/S1\n",
    "        max_modularity -= temp_sum\n",
    "        #print(temp_sum, max_modularity)\n",
    "        modularity[attr] += A_class.sum()-temp_sum\n",
    "    modularity[attr] /= max_modularity\n",
    "    #print(modularity[attr], max_modularity)\n",
    "    \n",
    "print(modularity)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16389241]]\n",
      "{1: array([-0.878421]), 2: array([-0.02763613]), 4: array([-0.35215334]), 5: array([-0.19798115]), 6: array([-0.00792501])}\n"
     ]
    }
   ],
   "source": [
    "data2 = scipy.io.loadmat('Texas80.mat')\n",
    "A2 = data2['A']\n",
    "local_info = data2['local_info']\n",
    "num_vertices2, num_attributes = local_info.shape\n",
    "k = np.array(A2.sum(1))\n",
    "S1 = k.sum()\n",
    "S2 = (k*k).sum()\n",
    "S3 = (k*k*k).sum()\n",
    "Se = np.matrix(k).transpose()*A2*np.matrix(k)\n",
    "r = (Se*S1-S2*S2)/(S3*S1-S2*S2)\n",
    "print(r)\n",
    "attrib_check = [1,2,4,5,6]\n",
    "modularity = {1:0,2:0,4:0,5:0,6:0}\n",
    "for attr in attrib_check:\n",
    "    attr_classes = {}\n",
    "    #print(attr)\n",
    "    for j in range(num_vertices2):\n",
    "        try:\n",
    "            attr_classes[local_info[j, attr]].append(j)\n",
    "        except:\n",
    "            attr_classes[local_info[j, attr]] = [j]\n",
    "    #print(len(attr_classes))\n",
    "    max_modularity = S1\n",
    "    for attr_v, ind_list in attr_classes.items():\n",
    "        k_class = k[ind_list]\n",
    "        A_class = A2[ind_list,ind_list]\n",
    "        temp_sum = sum(k_class)**2/S1\n",
    "        max_modularity -= temp_sum\n",
    "        #print(temp_sum, max_modularity)\n",
    "        modularity[attr] += A_class.sum()-temp_sum\n",
    "    modularity[attr] /= max_modularity\n",
    "    #print(modularity[attr], max_modularity)\n",
    "    \n",
    "print(modularity)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, node degree assortativity is usually approximately equal to 0, so the same number of friends is not a reason for being related. Also we can see that all modularities are below zero, which means that in our cases we have Disassortative mixing (users with different attributes are tend to be related), especially by gender."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
